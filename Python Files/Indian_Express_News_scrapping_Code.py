# -*- coding: utf-8 -*-
"""Task_2_Aayush .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ViiHMCdAvPM3RErMSBBxRzkzvVnSUWdY
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
from time import sleep
import re

# base url
url='https://indianexpress.com/section/business/'
page=requests.get(url)
soup1=BeautifulSoup(page.content, 'html.parser')
soup2=BeautifulSoup(soup1.prettify(), 'html.parser')
container = soup2.find('div', class_ = 'articles')

all_links = []
# These are webpages you can increase it by increasing the length right now i am using 4
for i in range(1, 5):
    try:
        url = f"https://indianexpress.com/section/business/page/{i}/"
        page = requests.get(url)
        page.raise_for_status()  # Check if the request was successful
        sleep(0.05)

        # Create BeautifulSoup object to parse the HTML
        soup1 = BeautifulSoup(page.content, "html.parser")
        soup2 = BeautifulSoup(soup1.prettify(), "html.parser")
        sleep(0.05)

        containers = soup2.find_all('div', class_='articles')
        sleep(0.05)

        # There are many articles on the page, this will store the link for all pages
        links = []
        for container in containers:
            link_element = container.find('a')
            if link_element:  # Check if link exists before extracting href
                links.append(link_element['href'])
            sleep(0.05)

        # Add the links from this page to the all_links list
        all_links.extend(links)
        print(f'Page {i} done')
    except requests.exceptions.RequestException as e:
        print(f"Request error on page {i}: {e}")
    except Exception as e:
        print(f"Error on page {i}: {e}")

table = []

for link in all_links:
    try:
        # Check if the link contains "live update" (case-insensitive)
        if "live-update" in link.lower():
            print(f"Skipping link {link} because it contains 'live update'")
            continue

        url = link
        page = requests.get(url)
        page.raise_for_status()  # Check if the request was successful
        sleep(0.05)

        soup1 = BeautifulSoup(page.content, "html.parser")
        soup2 = BeautifulSoup(soup1.prettify(), "html.parser")

        titles = []
        authors = []
        dates = []
        content = []

        # Extracting the title
        titletag = soup2.find(class_='native_story_title')
        title = titletag.text.strip() if titletag else "Title not found"

        titles.append(title)

        # Extracting author and date information
        datandtimetag = soup2.find('div', class_='editor-share')
        if datandtimetag:
            author_tag = datandtimetag.find('a', href=True, rel='noamphtml')
            author = author_tag.get_text(strip=True) if author_tag else "Author not found"

            date_tag = datandtimetag.find('span', itemprop='dateModified')
            date = date_tag.get_text(strip=True) if date_tag else "Date not found"
        else:
            author = "Author not found"
            date = "Date not found"

        authors.append(author)
        dates.append(date)

        # Extracting the story content
        story = soup2.find('div', class_='story_details', id='pcl-full-content')
        if story:
            paragraphs = story.find_all('p')
            if paragraphs:
                paragraphs.pop()  # Remove the last <p> tag if it exists because it is a sponsored ad

            # Extract the text from the remaining <p> tags
            text_content = ' '.join([p.get_text(strip=True) for p in paragraphs])
        else:
            text_content = "Content not found"

        content.append(text_content)

        table.append([titles, authors, dates, content])
        print(f'Successfully processed link: {url}')
        sleep(0.05)

    except requests.exceptions.RequestException as e:
        print(f"Request error for link {url}: {e}")
    except Exception as e:
        print(f"Error processing link {url}: {e}")

table_headers = ['title', 'author', 'date_and_time', 'content']

table_df = pd.DataFrame(table, columns=table_headers)
# this will remove the square bracets by converting list to str.
for i in table_df:
  table_df[i]=table_df[i].apply(lambda x: ' '.join(x))

df=table_df

df

"""SQL Part"""

# Function to create the SQLite database schema.
import sqlite3

def create_database_schema(db_name):
    """
    Create the database schema for the articles table.

    Parameters:
    - db_name (str): The name of the database file. Default is 'articles.db'.
    """
    try:
        # Connect to the SQLite database (or create it if it doesn't exist)
        conn = sqlite3.connect(db_name)
        cursor = conn.cursor()

        # Define the CREATE TABLE statement with detailed schema
        create_table_sql = '''
        CREATE TABLE IF NOT EXISTS article (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            author TEXT,
            publication_date TEXT NOT NULL,
            content TEXT NOT NULL
        )
        '''

        # Execute the CREATE TABLE statement
        cursor.execute(create_table_sql)

        # Commit the transaction
        conn.commit()

        print("Table 'article' created successfully in database:", db_name)

    except sqlite3.Error as e:
        print("SQLite error:", e)

    finally:
        # Close the connection
        if conn:
            conn.close()

create_database_schema('articles')

conn = sqlite3.connect('articles')
cursor = conn.cursor()

# insert the data from the DataFrame into the articles table
for index, row in table_df.iterrows():
    cursor.execute('''
        INSERT INTO article (title, author, publication_date, content)
        VALUES (?,?,?,?)
    ''', (row['title'], row['author'], row['date_and_time'], row['content']))

# commit the changes and close the connection
conn.commit()
conn.close()

conn = sqlite3.connect('articles')
cursor = conn.cursor()
cursor.execute("SELECT * FROM article")
rows = cursor.fetchall()
column_names = [description[0] for description in cursor.description]
print(f"Column names: {column_names}")

# Print each row
for row in rows:
    print(row)

# Close the connection
conn.close()